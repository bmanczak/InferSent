{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5aabe9",
   "metadata": {},
   "source": [
    "# Demo notebook\n",
    "\n",
    "In this notebook you will be able to run the model on the SNLI dataset and see some shotcomings of different models.\n",
    "\n",
    "Let's get to it!\n",
    "First let's import the necessary libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec34f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field\n",
    "from torchtext.datasets import SNLI\n",
    "\n",
    "from train import InferSent\n",
    "from dataset import create_iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c4656",
   "metadata": {},
   "source": [
    "Load the data the Field operator that will enables us to preprocess the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60474e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/acts/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/opt/miniconda3/envs/acts/lib/python3.7/site-packages/torchtext/data/example.py:13: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/opt/miniconda3/envs/acts/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT, LABEL, train_iter, val_iter, test_iter = create_iterators(batch_size = 4, return_label = True) # feel free to change the batch size to your computer's capabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad6456",
   "metadata": {},
   "source": [
    "Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a390869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Freezing the embeddings\n"
     ]
    }
   ],
   "source": [
    "biLSTMPool_PATH = \"/Users/blazejmanczak/Desktop/School/Year1/Block5/acts/Practical/saved_models/biLstmPool/biLstmPool824.ckpt\"\n",
    "biLstmPool = InferSent.load_from_checkpoint(biLSTMPool_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab811393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Freezing the embeddings\n"
     ]
    }
   ],
   "source": [
    "uniLSTM_PATH =\"saved_models/uniLSTM/uniLSTM_8152.ckpt\"\n",
    "uniLstm = InferSent.load_from_checkpoint(uniLSTM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5248235",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs_PATH = \"saved_models/word_embs/word_embs642.ckpt\"\n",
    "#hparams_file = \"saved_models/word_embs/hparams.yaml\"\n",
    "word_embs = InferSent.load_from_checkpoint(checkpoint_path = word_embs_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217eb77",
   "metadata": {},
   "source": [
    "## Model evaluation \n",
    "\n",
    "We evalaute the model on SNLI task as well as the SentEval framework.\n",
    "By running `python eval.py --checkpoint_path` we can obtain the results on the SentEval dataset.\n",
    "\n",
    "An example evaluation in SNLI can be found below.\n",
    "For the sake of time, below please find the table with the results below:\n",
    "\n",
    "| Model | dim |NLI dev | NLI test | Transfer micro | Transfer macro |\n",
    "| ---   | --- | ---  | --- | --- | --- |\n",
    "| AWE |  | 300| 0.64 | 84.12 | 79.78 |\n",
    "| uniLSTM | 2048 | 0.64| 0.64 | 83.38 | 80.05 |\n",
    "| bi-LSTM | 4096 | 0.64| 0.64 | 85.94 | 82.63 |\n",
    "| biLSTM-max | 4096 | 82.56| 82.40 | 86.98 | 83.82 \n",
    "\n",
    "One can see that the for NLI the test accuracies are very close to the validation results. This might be caused by conservative early stopping with a patience of only 3. Once one looks at the training plots we see that training for a couple more epochs might have proven beneficial for the SNLI dataset.\n",
    "\n",
    "However, this early stopping seems to benefit the performance on the transfer tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06814f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWE, uni-LSTM, bi-LSTM, bi-LSTM Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7259303",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model evaluation - SNLI\n",
    "\n",
    "def evaluate_SNLI(model, iterator):\n",
    "    \n",
    "    acc = 0\n",
    "    count = 0 # of examples\n",
    "    \n",
    "    for batch in iterator:\n",
    "        preds = model(batch)\n",
    "        labels = batch.label - 1\n",
    "        acc += (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        count += batch.premise[1].shape[0] # add the batch size\n",
    "        \n",
    "    return acc/count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/acts/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "evaluate_SNLI(word_embs, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4332aa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x7f835e758d10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24aa281",
   "metadata": {},
   "source": [
    "### Model evaluation - SentEval\n",
    "\n",
    "By running `python eval.py --checkpoint_path your_checkpoint_path` we get the dictionary with model performance for each dataset. To get the micro and macro aggregate scores we can run the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d4b487",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentEval_awe = {\n",
    "                    'MR': {'devacc': 63.29, 'acc': 61.43, 'ndev': 74, 'ntest': 74},\n",
    "                    'CR': {'devacc': 80.18, 'acc': 80.69, 'ndev': 3775, 'ntest': 3775},\n",
    "                    'MPQA': {'devacc': 87.88, 'acc': 87.76, 'ndev': 10606, 'ntest': 10606},\n",
    "                    'SUBJ': {'devacc': 99.6, 'acc': 99.6, 'ndev': 5020, 'ntest': 5020},\n",
    "                    'SST2': {'devacc': 78.67, 'acc': 79.74, 'ndev': 872, 'ntest': 1821},\n",
    "                    'TREC': {'devacc': 75.09, 'acc': 84.0, 'ndev': 5452, 'ntest': 500},\n",
    "                    'MRPC': {'devacc': 72.94, 'acc': 72.12, 'f1': 80.84, 'ndev': 4076, 'ntest': 1725},\n",
    "                    'SICKEntailment': {'devacc': 80.6, 'acc': 78.2, 'ndev': 500, 'ntest': 4927},\n",
    "                    'SICKRelatedness': {'devpearson': 0.7978672893567289, 'pearson': 0.7992130424967328,\n",
    "                                        'spearman': 0.7187625218088702, 'mse': 0.36772601686375367,'ndev': 500, 'ntest': 4927}}\n",
    "\n",
    "sentEval_uni = {\n",
    "                   'MR': {'devacc': 68.36, 'acc': 64.82, 'ndev': 74, 'ntest': 74},\n",
    "                   'CR': {'devacc': 79.12, 'acc': 78.54, 'ndev': 3775, 'ntest': 3775},\n",
    "                   'MPQA': {'devacc': 88.13, 'acc': 88.25, 'ndev': 10606, 'ntest': 10606},\n",
    "                   'SUBJ': {'devacc': 99.61, 'acc': 99.58, 'ndev': 5020, 'ntest': 5020},\n",
    "                   'SST2': {'devacc': 78.21, 'acc': 79.3, 'ndev': 872, 'ntest': 1821},\n",
    "                   'TREC': {'devacc': 71.0, 'acc': 82.4, 'ndev': 5452, 'ntest': 500},\n",
    "                   'MRPC': {'devacc': 72.96, 'acc': 71.25, 'f1': 79.76, 'ndev': 4076, 'ntest': 1725}, \n",
    "                   'SICKEntailment': {'devacc': 83.0, 'acc': 84.49, 'ndev': 500, 'ntest': 4927}, \n",
    "                   'SICKRelatedness': {'devpearson': 0.8571529614695601, 'pearson': 0.8623872347038861,\n",
    "                                       'spearman': 0.798903697075263, 'mse': 0.26324771018968957, 'ndev': 500, 'ntest': 4927}}\n",
    "sentEval_bi = {\n",
    "                    'MR': {'devacc': 71.22, 'acc': 75.71, 'ndev': 74, 'ntest': 74},\n",
    "                    'CR': {'devacc': 79.78, 'acc': 79.87, 'ndev': 3775, 'ntest': 3775},\n",
    "                    'MPQA': {'devacc': 88.11, 'acc': 87.93, 'ndev': 10606, 'ntest': 10606},\n",
    "                    'SUBJ': {'devacc': 99.6, 'acc': 99.6, 'ndev': 5020, 'ntest': 5020},\n",
    "                    'SST2': {'devacc': 79.13, 'acc': 80.12, 'ndev': 872, 'ntest': 1821},\n",
    "                    'TREC': {'devacc': 83.33, 'acc': 88.4, 'ndev': 5452, 'ntest': 500},\n",
    "                    'MRPC': {'devacc': 74.44, 'acc': 71.88, 'f1': 80.34, 'ndev': 4076, 'ntest': 1725},\n",
    "                    'SICKEntailment': {'devacc': 85.4, 'acc': 84.11, 'ndev': 500, 'ntest': 4927},\n",
    "                    'SICKRelatedness': {'devpearson': 0.8673686615426182, 'pearson': 0.871446993869367,\n",
    "                                        'spearman': 0.8117278599098882, 'mse': 0.24626234335946046, 'ndev': 500, 'ntest': 4927}}\n",
    "\n",
    "sentEval_biPool = {\n",
    "                    'MR': {'devacc': 72.74, 'acc': 70.71, 'ndev': 74, 'ntest': 74},\n",
    "                     'CR': {'devacc': 82.58, 'acc': 82.3, 'ndev': 3775, 'ntest': 3775},\n",
    "                     'MPQA': {'devacc': 88.88, 'acc': 89.04, 'ndev': 10606, 'ntest': 10606},\n",
    "                     'SUBJ': {'devacc': 99.6, 'acc': 99.6, 'ndev': 5020, 'ntest': 5020},\n",
    "                     'SST2': {'devacc': 80.62, 'acc': 81.05, 'ndev': 872, 'ntest': 1821},\n",
    "                     'TREC': {'devacc': 84.78, 'acc': 89.2, 'ndev': 5452, 'ntest': 500},\n",
    "                     'MRPC': {'devacc': 75.19, 'acc': 74.09, 'f1': 81.69, 'ndev': 4076, 'ntest': 1725},\n",
    "                     'SICKEntailment': {'devacc': 86.2, 'acc': 85.69, 'ndev': 500, 'ntest': 4927},\n",
    "                     'SICKRelatedness': {'devpearson': 0.8877437481402046, 'pearson': 0.8847378389815521,\n",
    "                                         'spearman': 0.8247103664125432, 'mse': 0.2215079583066481, 'ndev': 500, 'ntest': 4927}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7883a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline of avergiving the GloVe 840 word embeddings with SpaCy tokenization {'micro': 84.11537777777777, 'macro': 79.78125}\n",
      "Uni-directional LSTM Transfer accuracies {'micro': 83.37980905349795, 'macro': 80.04875}\n",
      "Bi-drectional LSTM Transfer accuracies {'micro': 85.93779094650208, 'macro': 82.62625}\n",
      "Bi-directional LSTM with max pooling Transfer accuracies {'micro': 86.97518288065842, 'macro': 83.82374999999999}\n"
     ]
    }
   ],
   "source": [
    "def eval_senteval_dic(dic):\n",
    "    \n",
    "    datasets = list(dic.keys())\n",
    "    \n",
    "    accs = []\n",
    "    counts = []\n",
    "    result = {\"micro\": 0, \"macro\": 0}\n",
    "    \n",
    "    for ds in datasets:\n",
    "        temp_dic = dic[ds]\n",
    "        \n",
    "        try:\n",
    "            accs.append(temp_dic[\"devacc\"])\n",
    "            counts.append(temp_dic[\"ndev\"])\n",
    "            #accs.append(temp_dic[\"acc\"])\n",
    "            #counts.append(temp_dic[\"ntest\"])\n",
    "        except: # for metrics that don't have \"dev_acc\" as a key\n",
    "            continue\n",
    "            \n",
    "    result[\"macro\"] = np.mean(accs)\n",
    "    result[\"micro\"] = np.average(accs, weights = counts)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Baseline of avergiving the GloVe 840 word embeddings with SpaCy tokenization\", eval_senteval_dic(sentEval_awe))\n",
    "print(\"Uni-directional LSTM Transfer accuracies\", eval_senteval_dic(sentEval_uni))\n",
    "print(\"Bi-drectional LSTM Transfer accuracies\", eval_senteval_dic(sentEval_bi))\n",
    "print(\"Bi-directional LSTM with max pooling Transfer accuracies\", eval_senteval_dic(sentEval_biPool))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6299ac49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonus: AWE without tokenization:  {'micro': 82.82142067344317, 'macro': 80.85249999999999}\n"
     ]
    }
   ],
   "source": [
    "## Of the record: AWE without tokenization\n",
    "dic = {\n",
    "    'MRPC': {'ntest': 1725, 'f1': 81.21, 'acc': 72.64, 'devacc': 72.82, 'ndev': 4076},\n",
    "    'CR': {'ndev': 3775, 'acc': 79.63, 'devacc': 80.29, 'ntest': 3775}, \n",
    "    'MPQA': {'ndev': 10606, 'acc': 88.0, 'devacc': 87.82, 'ntest': 10606},\n",
    "    'SICKEntailment': {'ndev': 500, 'acc': 79.01, 'devacc': 81.0, 'ntest': 4927},\n",
    "    'SST2': {'ndev': 872, 'acc': 79.85, 'devacc': 79.01, 'ntest': 1821}, \n",
    "    'SUBJ': {'ndev': 10000, 'acc': 91.69, 'devacc': 91.77, 'ntest': 10000}, \n",
    "    'MR': {'ndev': 10662, 'acc': 78.05, 'devacc': 78.01, 'ntest': 10662}, \n",
    "    'TREC': {'ndev': 5452, 'acc': 84.8, 'devacc': 76.1, 'ntest': 500}}\n",
    "\n",
    "transfer_tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC',\n",
    "                      'MRPC', 'SICKEntailment', \"SICKRelatedness\"]\n",
    "new_dic = {key:val for key,val in dic.items() if key in transfer_tasks }\n",
    "\n",
    "print(\"Bonus: AWE without tokenization: \", eval_senteval_dic(new_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce4da4",
   "metadata": {},
   "source": [
    "Somewhat unexpectedly we see that just averging the GloVe word embeeddings is a very strong baseline, performing similarily to the uni-directional LSTM.\n",
    "One should not that the difference in accuracies for SNLI is much larger. It shows that these highly parametrized models capture not only general-purpose sentence representation but also utilize some biases and artifiacts of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34733d",
   "metadata": {},
   "source": [
    "## Running the model on our own example\n",
    "It would be nice to see the model in action on an arbitrary example. For that we need a small utility function that preprocess the string we supply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42154d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(premise:str, hypothesis:str, model):\n",
    "    \"\"\"\n",
    "    Processes one example.\n",
    "    \"\"\"\n",
    "    \n",
    "    premise = TEXT.process([TEXT.preprocess(premise)])\n",
    "    hypothesis = TEXT.process([TEXT.preprocess(hypothesis)])\n",
    "    \n",
    "    d = edict({\"premise\": premise, \"hypothesis\":hypothesis})\n",
    "    result = model(d).argmax(dim=-1)\n",
    "    \n",
    "    return result, LABEL.vocab.itos[1:][result.item()] # [1:] to omit the <unk> token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384472dc",
   "metadata": {},
   "source": [
    "Now let's try it! Feel free to try your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"I like this course.\"\n",
    "hypothesis = \"Amsterdam is a pretty city of course.\"\n",
    "\n",
    "# premise = \"Your premise\"\n",
    "# hypothesis = \"Your hypothesis\"\n",
    "\n",
    "process_example(premise, hypothesis,\n",
    "                 model= word_embs)\n",
    "\n",
    "#process_example(premise, hypothesis, # kernel dies on my computer\n",
    "#                model= biLstmPool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613d5af",
   "metadata": {},
   "source": [
    "## Error analysis: quantative\n",
    "\n",
    "Different model architectures have an impact on shortcomings of certain models. One interesting analysis is the impact of the length of the hypothesis and premise on the models performance.\n",
    "\n",
    "Can our model reliably encode the unusally short/long sentences?\n",
    "Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf89691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2ea5b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise mean and std length  14.144338607788086 6.079199314117432\n",
      "Hypothesis mean and std length  8.275193214416504 3.232281446456909\n",
      "--------------------\n",
      "10 and 90 percentile of the premise length 8.0 22.0\n",
      "10 and 90 percentile of the premise length 5.0 12.0\n"
     ]
    }
   ],
   "source": [
    "lengths_premise = torch.Tensor([])\n",
    "lengths_hypothesis = torch.Tensor([])\n",
    "\n",
    "for batch in train_iter:\n",
    "    lengths_premise = torch.cat((lengths_premise, batch.premise[1]))\n",
    "    lengths_hypothesis = torch.cat((lengths_hypothesis, batch.hypothesis[1]))\n",
    "\n",
    "print(\"Premise mean and std length \", torch.mean(lengths_premise).item(), torch.std(lengths_premise).item())\n",
    "print(\"Hypothesis mean and std length \", torch.mean(lengths_hypothesis).item(), torch.std(lengths_hypothesis).item())\n",
    "\n",
    "print(\"-\"*20)\n",
    "print(\"10 and 90 percentile of the premise length\", np.percentile(lengths_premise.numpy(), 10), np.percentile(lengths_premise.numpy(), 90))\n",
    "print(\"10 and 90 percentile of the hypothesis length\", np.percentile(lengths_hypothesis.numpy(), 10), np.percentile(lengths_hypothesis.numpy(), 90))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d4a71945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(model, test_iter, threshold, length_source = \"premise\", comparison = \"smaller\"):\n",
    "    \"\"\"Calculates the accuracy for examples with lenght greater/smaller than threshold.\"\"\"\n",
    "\n",
    "    acc = 0\n",
    "    count = 0\n",
    "\n",
    "    for batch in test_iter:\n",
    "\n",
    "        if length_source == \"premise\":\n",
    "            if comparison == \"smaller\":\n",
    "                mask = batch.premise[1] <= threshold\n",
    "            else:\n",
    "                mask = batch.premise[1] >= threshold\n",
    "        else:\n",
    "            if comparison == \"smaller\":\n",
    "                mask = batch.hypothesis[1] <= threshold\n",
    "            else:\n",
    "                mask = batch.hypothesis[1] >= threshold\n",
    "\n",
    "        if any(mask):\n",
    "            batch = edict({\"premise\": (batch.premise[0][:, mask], batch.premise[1][mask]),\n",
    "                          \"hypothesis\": ( batch.hypothesis[0][:, mask], batch.hypothesis[1][mask] ),\n",
    "                          \"label\": batch.label[mask]})\n",
    "\n",
    "            count += sum(mask)\n",
    "\n",
    "            preds = model(batch).argmax(dim = -1)\n",
    "            labels = batch.label - 1\n",
    "\n",
    "            acc += (preds == labels).float().sum()\n",
    "\n",
    "        #if count > 1000:\n",
    "        #    break\n",
    "        \n",
    "    #print(\"Accuracy:\", (acc/count *100).item(), \"%\")\n",
    "    return (acc/count).item()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9412c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for model InferSent(\n",
      "  (model): LstmEncoders(\n",
      "    (embedding): Embedding(33672, 300)\n",
      "    (lstm): LSTM(300, 2048)\n",
      "  )\n",
      "  (loss_module): CrossEntropyLoss()\n",
      "  (dense): Linear(in_features=8192, out_features=512, bias=True)\n",
      "  (classify): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n",
      "Accuracy for parameters threshold=8,source=premise, comparison smaller\n",
      "Accuracy: 81.04906463623047 %\n",
      "tensor(0.8105)\n",
      "Accuracy for parameters threshold=22,source=premise, comparison larger\n",
      "Accuracy: 77.4307632446289 %\n",
      "tensor(0.7743)\n",
      "Accuracy for parameters threshold=5,source=hypothesis, comparison smaller\n",
      "Accuracy: 84.81548309326172 %\n",
      "tensor(0.8482)\n",
      "Accuracy for parameters threshold=12,source=hypothesis, comparison largerr\n",
      "Accuracy: 75.4619369506836 %\n",
      "tensor(0.7546)\n"
     ]
    }
   ],
   "source": [
    "models = [uniLstm]\n",
    "values = [8,22, 5, 12] # 10th percentile premise\n",
    "length_source = [\"premise\", \"premise\", \"hypothesis\", \"hypothesis\"]\n",
    "comparison = [\"smaller\", \"larger\", \"smaller\", \"largerr\"]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(\"Running for model\", model)\n",
    "    for threshold,source, comp in zip(values, length_source, comparison):\n",
    "        print(f\"Accuracy for parameters threshold={threshold},source={source}, comparison={comp}\")\n",
    "        print(masked_accuracy(model, test_iter, threshold, source, comp))\n",
    "    print(\"\\n\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a8509208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for model InferSent(\n",
      "  (model): AvgWordEmbeddings(\n",
      "    (embedding): Embedding(33635, 300)\n",
      "  )\n",
      "  (loss_module): CrossEntropyLoss()\n",
      "  (dense): Linear(in_features=1200, out_features=512, bias=True)\n",
      "  (classify): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n",
      "Accuracy for parameters threshold=8,source=premise, comparison=smaller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/acts/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.5516128540039 %\n",
      "tensor(0.6455)\n",
      "Accuracy for parameters threshold=22,source=premise, comparison=larger\n",
      "Accuracy: 63.05244445800781 %\n",
      "tensor(0.6305)\n",
      "Accuracy for parameters threshold=5,source=hypothesis, comparison=smaller\n",
      "Accuracy: 65.69873046875 %\n",
      "tensor(0.6570)\n",
      "Accuracy for parameters threshold=12,source=hypothesis, comparison=largerr\n",
      "Accuracy: 61.12343215942383 %\n",
      "tensor(0.6112)\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [word_embs]\n",
    "values = [8,22, 5, 12] # 10th percentile premise\n",
    "length_source = [\"premise\", \"premise\", \"hypothesis\", \"hypothesis\"]\n",
    "comparison = [\"smaller\", \"larger\", \"smaller\", \"largerr\"]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(\"Running for model\", model)\n",
    "    for threshold,source, comp in zip(values, length_source, comparison):\n",
    "        print(f\"Accuracy for parameters threshold={threshold},source={source}, comparison={comp}\")\n",
    "        print(masked_accuracy(model, test_iter, threshold, source, comp))\n",
    "    print(\"\\n\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b188c",
   "metadata": {},
   "source": [
    "For all models we see the general trend: for longer sentences, especially hypothesis, the models perform worse. This due to the fact that long-distance relationships are harder to encode and possibly because the longer sentences can be more convoluted.\n",
    "\n",
    "As expected, we see that the uni-directional LSTM performs worse than the bi-directional counterparts due to increased capacity of encoding longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc579e7",
   "metadata": {},
   "source": [
    "## Error analysis: qualitative\n",
    "\n",
    "Authors of InferSent paper hypothesised that the best performing model, bi-directional LSTM with max-pooling performed better because of the model's capability to make sharp choices on which part on which part of the sentence is more important for others.\n",
    "\n",
    "This can be tested by engineering examples that mostly point to one sentence relation but have a subtle part such as a negation or word sense ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f01fa0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7dc07b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2123\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "if a:\n",
    "    print(2123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7114ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec3009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4657f4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 13])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][:,mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bdc05403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1]), 'contradiction')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = \"A soccer game with multiple males playing\"\n",
    "hypothesis = \"Some men are playing sport which is not soccer\"\n",
    "\n",
    "premise = \"In due course we will know the exam grades\"\n",
    "hypothesis = \"We will have to wait forever for our course grades\"\n",
    "\n",
    "#premise = \"Men are playing a funny game involving smiling cats. \"\n",
    "#hypothesis = \"Two men are smiling and laughing at the cats playing on the floor.\"\n",
    "\n",
    "#premise = \"A man inspects the uniform of a figure in some East Asian country.\"\n",
    "#hypothesis = \"The man is sleeping\"\n",
    "\n",
    "\n",
    "process_example(premise, hypothesis,\n",
    "                model= uniLstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e30a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fa66ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7ff4ae3a40d0>>,\n",
       "            {'<unk>': 0, 'entailment': 1, 'contradiction': 2, 'neutral': 3})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b52165d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entailment', 'contradiction', 'neutral']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.itos[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f7638443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4590f471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), 'entailment')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = \"A soccer game with multiple males playing\"\n",
    "hypothesis = \"Some men are playing sport which is not soccer\"\n",
    "\n",
    "#premise = \"A man inspects the uniform of a figure in some East Asian country.\"\n",
    "#hypothesis = \"The man is sleeping\"\n",
    "\n",
    "#premise = \"He play piano\"\n",
    "#hypothesis = \"He does not play a piano\"\n",
    "\n",
    "process_example(premise, hypothesis,\n",
    "                model= uniLstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4ae7427",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'InferSent' object has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-dc5862e016bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muniLstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/acts/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'InferSent' object has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00074f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
